# Ecommerce duplicates detection

Finding near-duplicates in large datasets is an important problem for many online businesses. 
Millions of product listings are created daily on e-commerce platforms such as Amazon, Shopee or eBay. 
Then, the challenge for online shops is to identify similar products despite having completely different images or descriptions. 
Finding resembling products allows e-commerce platforms to recommend customers better postings and help them find a better deal, increasing the chances of purchasing and facilitating the listing process for retailers. 
This project aims to implement a solution for duplicate products detection exploring multiple approaches based on image and text embeddings and analysing their performance.

## Our Approach
Our overall approach was to generate image and text embeddings. Then these embeddings were used to retrieve possible duplicates. Finally we used neighborhood blending to make similar queries more distinct and closer to their neighbors.

The full report can be viewed ![here](https://github.com/pavponn/dl-ecommerce-duplicates/blob/master/Final_Report_Duplicate_Products_Detection_in_E_Commerce.pdf). 

### Embedding Generation
To establish similarity between two product listings, we adopt a classical approach of relying on the similarity between image and text (product title) embeddings. Initially,
we adopt a baseline approach where we extract embeddings from pre-trained image and text models, followed by fine-tuning using multiple loss functions that showed promising results in related work. To get the best results, we performed a hyperparameter search using techniques such as grid and random search, with the help of the Ray Tune framework.

### Baseline
Initially, pre-trained models were utilized without specific fine-tuning for the task. For image embeddings, ResNet-18 and DenseNet-121 were experimented with, chosen for their size and performance on the ImageNet dataset. 512-dimensional embeddings were extracted from both models. The text embeddings began with TF-IDF vectorization, followed by the use of BERT and an advanced sentence transformer model called paraphrase-xlm-r-multilingual-v1. The latter is designed for diverse languages, which was advantageous for data primarily from Southeast Asia and Taiwan. BERT has 12 transformer blocks and 110 million parameters, while XLM has 12 transformer blocks and 550 million parameters, both generating 768-dimensional vectors. Implementation details can be found in the "ResNet18Baseline.ipynb" and "textBaseline.ipynb" notebooks.

### Loss Functions
We tested multiple different loss functions combined with multiple pre-trained models to generate both text and image embeddings. For models we used XLM, BERT, and for image embeddings we used DenseNet-121 and ResNet-18.

The first loss function was **Contrastive Loss**. It is commonly used in siamese network architectures. It quantifies the similarity or dissimilarity between two input instances by increasing the distance between dissimilar ones and minimizing it for similar instances in the embedding space. The loss function involves calculating the distance between two object embeddings, X1 and X2, and is defined with respect to a predefined margin, m. The goal was to enhance the discriminative power of embeddings, aiding in the clustering of similar products. Implementation required creating pairwise training and validation datasets, building a siamese network, and using cosine similarity to determine duplicate listings. Detailed implementations can be found in "ImageContrastiveLossMethod.ipynb" and "TextContrastiveLossMethod.ipynb" notebooks.

The next loss function was **Triplet Loss**. Triplet loss is a loss function used in deep learning networks to create embeddings that map images or texts into a space where distances between points represent their similarity. It utilizes three types of embeddings: anchor (being learned), positive (similar to anchor), and negative (dissimilar to anchor). The loss aims to ensure that the distance between the anchor and positive is smaller than that between the anchor and negative, encouraging the network to produce semantically similar embeddings. The loss function includes a predefined margin, "m," and is implemented by creating a dataset of triplets and adding layers to pre-trained networks. Detailed implementations can be found in "TextXLMTripletLoss.iynb" and "ImagetResNetTripletLoss.ipynb" notebooks.

Next we used **Cross-Entropy Loss**. Cross-entropy is a widely used loss function in machine learning and deep learning, particularly for classification tasks. It quantifies how well a model's predictions align with the actual labels by measuring the negative log likelihood of each class. This differentiable loss function is well-suited for gradient descent optimization in neural networks. It was applied to both text and image baseline models before implementing ArcFace. You can find detailed implementations in "TextBERTArcfaceLoss.ipynb," "TextXLMArcfaceLoss.ipynb," and "ImageCrossEntropyLossMethod.ipynb" notebooks.

One of the more interesting loss functions was **ArcFace**. ArcFace is a technique used to enhance the discriminative capabilities of deep learning models by optimizing the softmax loss with an angular margin. This margin encourages the network to learn features that are more separable, particularly useful for distinguishing between similar classes, such as different faces. The ArcFace loss function adds an angular margin to the SoftMax loss, enhancing the model's effectiveness in class separation. Implementation involves using a pre-trained model as the backbone, adding batch normalization and dropout layers, and calculating the loss with the ArcFace loss function during training. Detailed implementations can be found in "ImageArcFaceLossMethod.ipynb," "TextBERTArcFaceLoss.ipynb," and "TextXLMArcFaceLoss.ipynb" notebooks.

Lastly, we used **CurricularFace** which is a modification of the Curriculum Loss function designed for training facial recognition models. Unlike traditional Curriculum Loss, CurricularFace dynamically organizes the training samples within mini-batches. It adjusts the importance of hard samples based on their specific difficulty in each mini-batch, optimizing the feature margin between different classes. The loss function is similar to ArcFace but modulates both positive and negative cosine similarities. Implementation involves creating a CurricularFace PyTorch module and adding it to baseline BERT, XLM, ResNet, and DenseNet models for text and image embeddings. Additional batch normalization and dropout layers are incorporated, and optimal parameters are determined through grid search. Detailed training and analysis can be found in "CurricularFaceLossImage.ipynb" and "CurricularFaceLossText.ipynb."

### Neighborhood Blending
Neighborhood Blending is a technique that helps to make similar queries more distinct and closer to their neighbors. This results in more coherent clusters. This happens by removing nodes that donâ€™t meet the similarity threshold criteria.

## Results
![Table 1](https://github.com/pavponn/dl-ecommerce-duplicates/blob/master/table1.JPG)
![Table 2](https://github.com/pavponn/dl-ecommerce-duplicates/blob/master/table2.JPG)

In the evaluation of various approaches for generating image and text embeddings, several key findings emerged. Image baselines, such as ResNet-18 and DenseNet-121, demonstrated solid performance without specific training. Text baselines, including TFIDF, BERT, and XLM, showed promise, with XLM outperforming BERT. The introduction of cross-entropy loss significantly boosted F1-scores for text embeddings but yielded limited improvements for images. Contrastive and triplet loss functions, despite extensive hyperparameter tuning, struggled to surpass baseline results. DenseNet-121 with ArcFace loss excelled in image embeddings, while ArcFace with XLM and BERT dominated in text embeddings, albeit with signs of potential overfitting. CurricularFace exhibited promise for text embeddings but faced challenges with images. It's worth noting that certain loss functions appeared to overfit to training data but still enhanced embeddings' expressiveness. Further cross-validation and analysis are recommended for a comprehensive assessment of model performance.
